[
  {
    "objectID": "posts/4classification/index.html",
    "href": "posts/4classification/index.html",
    "title": "Classification",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/1probability/index.html",
    "href": "posts/1probability/index.html",
    "title": "Probability theory and random variables",
    "section": "",
    "text": "Probability theory, a branch of mathematics concerned with the analysis of random phenomena. The outcome of a random event cannot be determined before it occurs, but it may be any one of several possible outcomes. The actual outcome is considered to be determined by chance.\nA random variable is a variable whose value is unknown or a function that assigns values to each of an experiment’s outcomes. Random variables are often designated by letters and can be classified as discrete, which are variables that have specific values, or continuous, which are variables that can have any values within a continuous range.\nLet’s take an example of the coin flips. We’ll start with flipping a coin and finding out the probability. We’ll use H for ‘heads’ and T for ‘tails’. So now we flip our coin 100 times, and we want to answer some questions. What is the probability of getting heads over these 100 times? Will the probability change when we flip from 1 time to 100 times?\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate_coin_flips(num_flips):\n    outcomes = np.random.choice(['Heads', 'Tails'], size=num_flips)\n    head_count = np.cumsum(outcomes == 'Heads')\n    probabilities = head_count / np.arange(1, num_flips + 1)\n    return probabilities\n\ndef plot_probability_change(probabilities):\n    plt.plot(probabilities)\n    plt.title('Probability of Getting Heads over 100 Coin Flips')\n    plt.xlabel('Number of Flips')\n    plt.ylabel('Probability')\n    plt.axhline(0.5, color='red', linestyle='--', label='Expected Probability (0.5)')\n    plt.legend()\n    plt.show()\n\n# Simulate coin flips and plot the results\nnum_flips = 1000\nprobabilities = simulate_coin_flips(num_flips)\nplot_probability_change(probabilities)\n\n\n\n\nFigure 1: flip coins\n\n\n\n\nBased on the above figure Figure 1, we can find that at the early stage, the probability of getting heads may be 0.3, 0.6 or something else. However, when we flip coins for more and more times, the probability will be converged to about the expected probability 0.5.\nThen next question we want to explore is that: What if we do some tricks to make the head side of coin heavier than the tail side of coin? In this case, let us assume head side of coin is 7 grams while the tail side is 3 grams.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate_coin_flips(num_flips):\n    outcomes = np.random.choice(['Heads', 'Tails'], size=num_flips,p=[0.7,0.3])\n    head_count = np.cumsum(outcomes == 'Heads')\n    probabilities = head_count / np.arange(1, num_flips + 1)\n    return probabilities\n\ndef plot_probability_change(probabilities):\n    plt.plot(probabilities)\n    plt.title('Probability of Getting Heads over 100 Coin Flips')\n    plt.xlabel('Number of Flips')\n    plt.ylabel('Probability')\n    plt.axhline(0.7, color='red', linestyle='--', label='Expected Probability (0.5)')\n    plt.legend()\n    plt.show()\n\n# Simulate coin flips and plot the results\nnum_flips = 3000\nprobabilities = simulate_coin_flips(num_flips)\nplot_probability_change(probabilities)\n\n\n\n\nFigure 2: flip coins\n\n\n\n\nAgain, we can find that at the early stage, the probability of getting heads may be something random. However, when we flip coins for more and more times, the probability will be converged to about the expected probability 0.7."
  },
  {
    "objectID": "posts/2cluster/index.html",
    "href": "posts/2cluster/index.html",
    "title": "Clustering",
    "section": "",
    "text": "Clustering is a set of techniques used to partition data into groups, or clusters. Clusters are loosely defined as groups of data objects that are more similar to other objects in their cluster than they are to data objects in other clusters.\nThe k-means clustering method is an unsupervised machine learning technique used to identify clusters of data objects in a dataset. There are many different types of clustering methods, but k-means is one of the oldest and most approachable. These traits make implementing k-means clustering in Python reasonably straightforward, even for novice programmers and data scientists.\n\nimport matplotlib.pyplot as plt\nfrom kneed import KneeLocator\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\nfrom sklearn.metrics import adjusted_rand_score\n\nscaler = StandardScaler()\n\nfeatures, true_labels = make_moons(\n      n_samples=250, noise=0.05, random_state=42\n  )\nscaled_features = scaler.fit_transform(features)\n# Instantiate k-means and dbscan algorithms\nkmeans = KMeans(n_clusters=2)\ndbscan = DBSCAN(eps=0.3)\n\n# Fit the algorithms to the features\nkmeans.fit(scaled_features)\ndbscan.fit(scaled_features)\n\n# Compute the silhouette scores for each algorithm\nkmeans_silhouette = silhouette_score(\n    scaled_features, kmeans.labels_\n).round(2)\ndbscan_silhouette = silhouette_score(\n    scaled_features, dbscan.labels_\n).round (2)\n# Plot the data and cluster silhouette comparison\nfig, (ax1, ax2) = plt.subplots(\n    1, 2, figsize=(8, 6), sharex=True, sharey=True\n)\nfig.suptitle(f\"Clustering Algorithm Comparison: Crescents\", fontsize=16)\nfte_colors = {\n    0: \"#008fd5\",\n    1: \"#fc4f30\",\n}\n# The k-means plot\nkm_colors = [fte_colors[label] for label in kmeans.labels_]\nax1.scatter(scaled_features[:, 0], scaled_features[:, 1], c=km_colors)\nax1.set_title(\n    f\"k-means\\nSilhouette: {kmeans_silhouette}\", fontdict={\"fontsize\": 12}\n)\n\n# The dbscan plot\ndb_colors = [fte_colors[label] for label in dbscan.labels_]\nax2.scatter(scaled_features[:, 0], scaled_features[:, 1], c=db_colors)\nax2.set_title(\n    f\"DBSCAN\\nSilhouette: {dbscan_silhouette}\", fontdict={\"fontsize\": 12}\n)\nplt.show()\n\n/home/user/software/anaconda3/envs/torch/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "homework for CS_5805",
    "section": "",
    "text": "Anomaly/outlier detection\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 5, 2023\n\n\nBarry Menglong Yao\n\n\n\n\n\n\n  \n\n\n\n\nClassification\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2023\n\n\nBarry Menglong Yao\n\n\n\n\n\n\n  \n\n\n\n\nLinear and nonlinear regression\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 3, 2023\n\n\nBarry Menglong Yao\n\n\n\n\n\n\n  \n\n\n\n\nClustering\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nBarry Menglong Yao\n\n\n\n\n\n\n  \n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nBarry Menglong Yao\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/5detection/index.html",
    "href": "posts/5detection/index.html",
    "title": "Anomaly/outlier detection",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/3linear/index.html",
    "href": "posts/3linear/index.html",
    "title": "Linear and nonlinear regression",
    "section": "",
    "text": "This is a post with executable code."
  }
]