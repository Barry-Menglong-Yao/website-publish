[
  {
    "objectID": "posts/4classification/index.html",
    "href": "posts/4classification/index.html",
    "title": "4. Classification",
    "section": "",
    "text": "Random forests are a popular supervised machine learning algorithm.\nRandom forests are for supervised machine learning, where there is a labeled target variable.\nRandom forests can be used for solving regression (numeric target variable) and classification (categorical target variable) problems.\nRandom forests are an ensemble method, meaning they combine predictions from other models.\nEach of the smaller models in the random forest ensemble is a decision tree.\nImagine you have a complex problem to solve, and you gather a group of experts from different fields to provide their input. Each expert provides their opinion based on their expertise and experience. Then, the experts would vote to arrive at a final decision.\nIn a random forest classification, multiple decision trees are created using different random subsets of the data and features. Each decision tree is like an expert, providing its opinion on how to classify the data. Predictions are made by calculating the prediction for each decision tree, then taking the most popular result. (For regression, predictions use an averaging technique instead.)\nThe Dataset:\nThis dataset consists of direct marketing campaigns by a Portuguese banking institution using phone calls. The campaigns aimed to sell subscriptions to a bank term deposit. We are going to store this dataset in a variable called bank_data.\nThe columns we will use are:\nage: The age of the person who received the phone call\ndefault: Whether the person has credit in default\ncons.price.idx: Consumer price index score at the time of the call\ncons.conf.idx:Consumer confidence index score at the time of the call\ny: Whether the person subscribed (this is what we’re trying to predict)\n\n\nCode\nfrom ucimlrepo import fetch_ucirepo \nimport pandas as pd\nimport numpy as np\nimport sklearn\nfrom scipy.stats import randint\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, ConfusionMatrixDisplay\nfrom sklearn.tree import export_graphviz\nfrom IPython.display import Image\nimport graphviz\nfrom sklearn.model_selection import RandomizedSearchCV, train_test_split\n  \n# fetch dataset \nbank_marketing = fetch_ucirepo(id=222) \n# alternatively: fetch_ucirepo(name='Heart Disease')\n\n  \n# data (as pandas dataframes) \ndf_X = bank_marketing.data.features \ndf_y = bank_marketing.data.targets \n\n\n\n\nCode\nX = df_X.loc[:,['age','default','balance']]\nX['default'] = X['default'].map({'no':0,'yes':1,'unknown':0})\ndf_y[\"y\"]=df_y[\"y\"].map({'no':0,'yes':1})\ny=df_y.loc[:,['y']]\n \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n\n/tmp/ipykernel_99544/2787111387.py:3: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\nCode\nrf = RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n# Create the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nConfusionMatrixDisplay(confusion_matrix=cm).plot()\n\n\n/home/user/software/anaconda3/envs/torch/lib/python3.10/site-packages/sklearn/base.py:1151: DataConversionWarning:\n\nA column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n\n\n\nAccuracy: 0.8392126506690257\n\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ff2acd7ef20&gt;\n\n\n\n\n\nLogistic regression is a process of modeling the probability of a discrete outcome given an input variable. The most common logistic regression models a binary outcome; something that can take two values such as true/false, yes/no, and so on. Multinomial logistic regression can model scenarios where there are more than two possible discrete outcomes. Logistic regression is a useful analysis method for classification problems, where you are trying to determine if a new sample fits best into a category. As aspects of cyber security are classification problems, such as attack detection, logistic regression is a useful analytic technique.\n\n\nCode\nfrom sklearn.linear_model import LogisticRegression\n\n# all parameters not specified are set to their defaults\nlogisticRegr = LogisticRegression()\nlogisticRegr.fit(X_train, y_train)\ny_pred = logisticRegr.predict(X_test)\naccuracy = accuracy_score(y_test, y_pred)\nprint(\"Accuracy:\", accuracy)\n# Create the confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nConfusionMatrixDisplay(confusion_matrix=cm).plot()\n\n\nAccuracy: 0.880128276014597\n\n\n/home/user/software/anaconda3/envs/torch/lib/python3.10/site-packages/sklearn/utils/validation.py:1184: DataConversionWarning:\n\nA column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n\n\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ff295515f00&gt;\n\n\n\n\n\nThe results show that both LogisticRegression and Random forests can reach a good accuracy."
  },
  {
    "objectID": "posts/1probability/index.html",
    "href": "posts/1probability/index.html",
    "title": "1. Probability theory and random variables",
    "section": "",
    "text": "Probability theory, a branch of mathematics concerned with the analysis of random phenomena. The outcome of a random event cannot be determined before it occurs, but it may be any one of several possible outcomes. The actual outcome is considered to be determined by chance.\nA random variable is a variable whose value is unknown or a function that assigns values to each of an experiment’s outcomes. Random variables are often designated by letters and can be classified as discrete, which are variables that have specific values, or continuous, which are variables that can have any values within a continuous range.\nLet’s take an example of the coin flips to show basic probability theory and random variables. We’ll start with flipping a coin and finding out the probability. We’ll use H for ‘heads’ and T for ‘tails’. So now we flip our coin 1000 times, and we want to answer some questions. What is the probability of getting heads over these 1000 times? Will the probability change when we flip from 1 time to 1000 times?\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate_coin_flips(num_flips):\n    outcomes = np.random.choice(['Heads', 'Tails'], size=num_flips)\n    head_count = np.cumsum(outcomes == 'Heads')\n    probabilities = head_count / np.arange(1, num_flips + 1)\n    return probabilities\n\ndef plot_probability_change(probabilities):\n    plt.plot(probabilities)\n    plt.title('Probability of Getting Heads over 100 Coin Flips')\n    plt.xlabel('Number of Flips')\n    plt.ylabel('Probability')\n    plt.axhline(0.5, color='red', linestyle='--', label='Expected Probability (0.5)')\n    plt.legend()\n    plt.show()\n\n# Simulate coin flips and plot the results\nnum_flips = 1000\nprobabilities = simulate_coin_flips(num_flips)\nplot_probability_change(probabilities)\n\n\n\n\n\nFigure 1: flip coins\n\n\n\n\nBased on the above figure Figure 1, we can find that at the early stage, the probability of getting heads may be 0.3, 0.6 or something else. However, when we flip coins for more and more times, the probability will be converged to the expected probability 0.5.\nThen next question we want to explore is that: What if we do some tricks to make the head side of coin heavier than the tail side of coin? In this case, let us assume head side of coin is 7 grams while the tail side is 3 grams.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef simulate_coin_flips(num_flips):\n    outcomes = np.random.choice(['Heads', 'Tails'], size=num_flips,p=[0.7,0.3])\n    head_count = np.cumsum(outcomes == 'Heads')\n    probabilities = head_count / np.arange(1, num_flips + 1)\n    return probabilities\n\ndef plot_probability_change(probabilities):\n    plt.plot(probabilities)\n    plt.title('Probability of Getting Heads over 100 Coin Flips')\n    plt.xlabel('Number of Flips')\n    plt.ylabel('Probability')\n    plt.axhline(0.7, color='red', linestyle='--', label='Expected Probability (0.5)')\n    plt.legend()\n    plt.show()\n\n# Simulate coin flips and plot the results\nnum_flips = 3000\nprobabilities = simulate_coin_flips(num_flips)\nplot_probability_change(probabilities)\n\n\n\n\n\nFigure 2: flip coins\n\n\n\n\nAgain, we can find that at the early stage, the probability of getting heads may be something random. However, when we flip coins for more and more times, the probability will be converged to the expected probability 0.7."
  },
  {
    "objectID": "posts/2cluster/index.html",
    "href": "posts/2cluster/index.html",
    "title": "2. Clustering",
    "section": "",
    "text": "Clustering is a set of techniques used to partition data into groups, or clusters. Clusters are loosely defined as groups of data objects that are more similar to other objects in their cluster than they are to data objects in other clusters.\nThe k-means clustering method is an unsupervised machine learning technique used to identify clusters of data objects in a dataset. There are many different types of clustering methods, but k-means is one of the oldest and most approachable. These traits make implementing k-means clustering in Python reasonably straightforward, even for novice programmers and data scientists.\nIn the following example, we use a simple toy dataset to visualize k-means and DBSCAN algorithms. make_moons() function produces two interleaving half circles.\n\n\nCode\nimport matplotlib.pyplot as plt\nfrom kneed import KneeLocator\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.datasets import make_moons\nfrom sklearn.metrics import adjusted_rand_score\n\nscaler = StandardScaler()\n\nfeatures, true_labels = make_moons(\n      n_samples=250, noise=0.05, random_state=42\n  )\nscaled_features = scaler.fit_transform(features)\n# Instantiate k-means and dbscan algorithms\nkmeans = KMeans(n_clusters=2)\ndbscan = DBSCAN(eps=0.3)\n\n# Fit the algorithms to the features\nkmeans.fit(scaled_features)\ndbscan.fit(scaled_features)\n\n# Compute the silhouette scores for each algorithm\nkmeans_silhouette = silhouette_score(\n    scaled_features, kmeans.labels_\n).round(2)\ndbscan_silhouette = silhouette_score(\n    scaled_features, dbscan.labels_\n).round (2)\n# Plot the data and cluster silhouette comparison\nfig, (ax1, ax2) = plt.subplots(\n    1, 2, figsize=(8, 6), sharex=True, sharey=True\n)\nfig.suptitle(f\"Clustering Algorithm Comparison: Crescents\", fontsize=16)\nfte_colors = {\n    0: \"#008fd5\",\n    1: \"#fc4f30\",\n}\n# The k-means plot\nkm_colors = [fte_colors[label] for label in kmeans.labels_]\nax1.scatter(scaled_features[:, 0], scaled_features[:, 1], c=km_colors)\nax1.set_title(\n    f\"k-means\\nSilhouette: {kmeans_silhouette}\", fontdict={\"fontsize\": 12}\n)\n\n# The dbscan plot\ndb_colors = [fte_colors[label] for label in dbscan.labels_]\nax2.scatter(scaled_features[:, 0], scaled_features[:, 1], c=db_colors)\nax2.set_title(\n    f\"DBSCAN\\nSilhouette: {dbscan_silhouette}\", fontdict={\"fontsize\": 12}\n)\nplt.show()\n\n\n/home/user/software/anaconda3/envs/torch/lib/python3.10/site-packages/sklearn/cluster/_kmeans.py:1412: FutureWarning:\n\nThe default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n\n\n\n\n\n\nWe can find that the k-means can cluster them into two groups while DBSCAN has a slightly better performance."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "homework for CS_5805",
    "section": "",
    "text": ":::{#quarto-listing-pipeline .hidden} \\(e = mC^2\\)\n:::{.hidden render-id=“pipeline-listing-listing”}\n:::{.list .quarto-listing-default}\n\n\n  \n\n\n\n\n\nClustering\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nBarry Menglong Yao\n\n\n\n\n\n\n  \n\n\n\n\n\nAnomaly/outlier detection\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nBarry Menglong Yao\n\n\n\n\n\n\n  \n\n\n\n\n\nProbability theory and random variables\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nBarry Menglong Yao\n\n\n\n\n\n\n  \n\n\n\n\n\nLinear and nonlinear regression\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nBarry Menglong Yao\n\n\n\n\n\n\n  \n\n\n\n\n\nClassification\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2023\n\n\nBarry Menglong Yao\n\n\n\n\n:::\n\n\n\n\n\n\nNo matching items\n\n:::\n:::"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/5detection/index.html",
    "href": "posts/5detection/index.html",
    "title": "5. Anomaly/outlier detection",
    "section": "",
    "text": "Anomalies are data points in a dataset that are different from the normal state of existence and contradict the data’s expected behavior. Anomalies in data are also called standard deviations, outliers, noise, novelties, and exceptions.\nThere are three types of anomalies:\nPoint anomalies: It is when a single instance of data is anomalous.\nContextual anomalies: It is when the abnormality is context-specific. It is common in time-series data.\nCollective anomalies: It is when a set of data instances collectively helps in detecting anomalies.\nIn machine learning and data mining, anomaly detection is the task of identifying the rare items, events or observations which are suspicious and seem different from the majority of the data. These anomalies can indicate some kind of problems such as bank fraud, medical problems, failure of industrial equipment, etc.\nLocal Outlier Factor: A local outlier factor is an algorithm that is used to find anomalous data points by measuring the local deviation of a given data point with respect to its neighbors.\nThe local outlier factor algorithm considers the distances of K-nearest neighbor’s from a core point to estimate the density. By comparing the local density of an object to the local densities of its neighbor’s, the regions of similar density and points that have a substantially lower density than their neighbor’s can be identified. These points are considered outliers by the algorithm. This is how the anomalies are detected by using this algorithm.\nIn the following example, we apply it on a real-world dataset. The dataset considered here contains information on 25 different features of Credit card payment defaults.\n\n\nCode\n# Import pandas\nimport pandas as pd\n\n# Load the data\ndata = pd.read_csv('https://raw.githubusercontent.com/analyticsindiamagazine/MocksDatasets/main/Credit_Card.csv')\n\n# Top 5 rows\ndata.head()\n\n\n\n\n\n\n\n\n\nID\nLIMIT_BAL\nSEX\nEDUCATION\nMARRIAGE\nAGE\nPAY_0\nPAY_2\nPAY_3\nPAY_4\n...\nBILL_AMT4\nBILL_AMT5\nBILL_AMT6\nPAY_AMT1\nPAY_AMT2\nPAY_AMT3\nPAY_AMT4\nPAY_AMT5\nPAY_AMT6\ndefault.payment.next.month\n\n\n\n\n0\n1\n20000.0\n2\n2\n1\n24\n2\n2\n-1\n-1\n...\n0.0\n0.0\n0.0\n0.0\n689.0\n0.0\n0.0\n0.0\n0.0\n1\n\n\n1\n2\n120000.0\n2\n2\n2\n26\n-1\n2\n0\n0\n...\n3272.0\n3455.0\n3261.0\n0.0\n1000.0\n1000.0\n1000.0\n0.0\n2000.0\n1\n\n\n2\n3\n90000.0\n2\n2\n2\n34\n0\n0\n0\n0\n...\n14331.0\n14948.0\n15549.0\n1518.0\n1500.0\n1000.0\n1000.0\n1000.0\n5000.0\n0\n\n\n3\n4\n50000.0\n2\n2\n1\n37\n0\n0\n0\n0\n...\n28314.0\n28959.0\n29547.0\n2000.0\n2019.0\n1200.0\n1100.0\n1069.0\n1000.0\n0\n\n\n4\n5\n50000.0\n1\n2\n1\n57\n-1\n0\n-1\n0\n...\n20940.0\n19146.0\n19131.0\n2000.0\n36681.0\n10000.0\n9000.0\n689.0\n679.0\n0\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n\nCode\n# Import IF and LOF\nfrom sklearn.ensemble import IsolationForest\nfrom sklearn.neighbors import LocalOutlierFactor\nimport matplotlib.pyplot as plt\nX=data[list(data.columns)] \n# Initialize and train LOF\nLOF = LocalOutlierFactor()\nLOF.fit(X)\n# Anomalies given by the LOF\nLOF_anomalies = LOF.fit_predict(X)\n\n\n\n\nCode\n# import matplotlib\nimport matplotlib.pyplot as plt\n# See how data is spreaded\nplt.figure(figsize=(8,6))\nplt.hist(X[\"AGE\"])\nplt.xlabel('Age')\nplt.ylabel('Frequency of datapoints')\n\n\nText(0, 0.5, 'Frequency of datapoints')\n\n\n\n\n\n\n\nCode\n# Plotting anomalies given by LOF\nplt.figure(figsize=(8,6))\nplt.hist(X[\"AGE\"],label='Normal')\nplt.hist(X[LOF_anomalies==-1][\"AGE\"], color='red', label='Anomalies')\nplt.xlabel('Age of the customers')\nplt.ylabel('Frequency')\nplt.ylim((0,6000))\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x7f66e2c95e70&gt;\n\n\n\n\n\nThe red part shows the detected anomalies data points around every age group."
  },
  {
    "objectID": "posts/3linear/index.html",
    "href": "posts/3linear/index.html",
    "title": "3. Linear and nonlinear regression",
    "section": "",
    "text": "Random forests are a popular supervised machine learning algorithm.\nRandom forests are for supervised machine learning, where there is a labeled target variable.\nRandom forests can be used for solving regression (numeric target variable) and classification (categorical target variable) problems.\nRandom forests are an ensemble method, meaning they combine predictions from other models.\nEach of the smaller models in the random forest ensemble is a decision tree.\nIn the following example we apply random forests and linear regression on the Temperature dataset.\nNote that the dataset is stored in the temps.csv file, which is also uploaded to our Github repository.\n\n\nCode\n# Pandas is used for data manipulation\nimport pandas as pd\n# Using Skicit-learn to split data into training and testing sets\nfrom sklearn.model_selection import train_test_split\n# Import matplotlib for plotting and use magic command for Jupyter Notebooks\nimport matplotlib.pyplot as plt\n# Import the model we are using\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\n \n    \n\ndef predict_by_random_forest(train_features, test_features, train_labels,feature_list):\n    # New random forest with only the two most important variables\n    rf_most_important = RandomForestRegressor(n_estimators= 1000, random_state=42)\n    # Extract the two most important features\n    important_indices = [feature_list.index('temp_1'), feature_list.index('average')]\n    train_important = train_features[:, important_indices]\n    test_important = test_features[:, important_indices]\n    # Train the random forest\n    rf_most_important.fit(train_important, train_labels)\n    # Make predictions and determine the error\n    predictions = rf_most_important.predict(test_important)\n    return predictions\n\n\ndef show_plot(features,feature_list,test_features,labels,predictions,test_labels):\n    errors = abs(predictions - test_labels)\n    # Display the performance metrics\n    print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')\n    mape = np.mean(100 * (errors / test_labels))\n    accuracy = 100 - mape\n    print('Accuracy:', round(accuracy, 2), '%.')\n    \n    # Set the style\n    plt.style.use('fivethirtyeight')\n    # Use datetime for creating date objects for plotting\n    import datetime\n    # Dates of training values\n    months = features[:, feature_list.index('month')]\n    days = features[:, feature_list.index('day')]\n    years = features[:, feature_list.index('year')]\n    # List and then convert to datetime object\n    dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n    dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in dates]\n    # Dataframe with true values and dates\n    true_data = pd.DataFrame(data = {'date': dates, 'actual': labels})\n    # Dates of predictions\n    months = test_features[:, feature_list.index('month')]\n    days = test_features[:, feature_list.index('day')]\n    years = test_features[:, feature_list.index('year')]\n    # Column of dates\n    test_dates = [str(int(year)) + '-' + str(int(month)) + '-' + str(int(day)) for year, month, day in zip(years, months, days)]\n    # Convert to datetime objects\n    test_dates = [datetime.datetime.strptime(date, '%Y-%m-%d') for date in test_dates]\n    # Dataframe with predictions and dates\n    predictions_data = pd.DataFrame(data = {'date': test_dates, 'prediction': predictions})\n    # Plot the actual values\n    plt.plot(true_data['date'], true_data['actual'], 'b-', label = 'actual')\n    # Plot the predicted values\n    plt.plot(predictions_data['date'], predictions_data['prediction'], 'ro', label = 'prediction')\n    plt.xticks(rotation = 60); \n    plt.legend()\n    # Graph labels\n    plt.xlabel('Date'); plt.ylabel('Maximum Temperature (F)'); plt.title('Actual and Predicted Values')\n    \n    \n# Read in data and display first 5 rows\nfeatures = pd.read_csv('/home/user/research/code/referred/course/website-publish/posts/3linear/temps.csv')\nfeatures.head(5)\n\n# One-hot encode the data using pandas get_dummies\nfeatures = pd.get_dummies(features)\n# Use numpy to convert to arrays\n\n# Labels are the values we want to predict\nlabels = np.array(features['actual'])\n# Remove the labels from the features\n# axis 1 refers to the columns\nfeatures= features.drop('actual', axis = 1)\n# Saving feature names for later use\nfeature_list = list(features.columns)\n# Convert to numpy array\nfeatures = np.array(features)\n\n# Split the data into training and testing sets\ntrain_features, test_features, train_labels, test_labels = train_test_split(features, labels, test_size = 0.25, random_state = 42)\npredictions=predict_by_random_forest(train_features, test_features, train_labels,feature_list)\n\nshow_plot(features,feature_list,test_features,labels,predictions,test_labels)\n\n\nMean Absolute Error: 3.92 degrees.\nAccuracy: 93.76 %.\n\n\n\n\n\nLinearRegression fits a linear model with coefficients w = (w1, …, wp) to minimize the residual sum of squares between the observed targets in the dataset, and the targets predicted by the linear approximation.\n\n\nCode\nfrom sklearn.linear_model import LinearRegression\n\n\n# Extract the two most important features\nimportant_indices = [feature_list.index('temp_1'), feature_list.index('average')]\ntrain_important = train_features[:, important_indices]\ntest_important = test_features[:, important_indices]\n# Train the random forest\nreg = LinearRegression().fit(train_important, train_labels)\n# Make predictions and determine the error\npredictions = reg.predict(test_important)\nshow_plot(features,feature_list,test_features,labels,predictions,test_labels)\n\n\nMean Absolute Error: 3.42 degrees.\nAccuracy: 94.63 %.\n\n\n\n\n\nThe results show that both LinearRegression and Random forests can reach a good performance."
  }
]